# Instructions

During your interaction with the user, if you find anything reusable in this project (e.g. version of a library, model name), especially about a fix to a mistake you made or a correction you received, you should take note in the `Lessons` section in the `.cursorrules` file so you will not make the same mistake again. 

You should also use the `.cursorrules` file as a Scratchpad to organize your thoughts. Especially when you receive a new task, you should first review the content of the Scratchpad, clear old different task if necessary, first explain the task, and plan the steps you need to take to complete the task. You can use todo markers to indicate the progress, e.g.
[X] Task 1
[ ] Task 2

Also update the progress of the task in the Scratchpad when you finish a subtask.
Especially when you finished a milestone, it will help to improve your depth of task accomplishment to use the Scratchpad to reflect and plan.
The goal is to help you maintain a big picture as well as the progress of the task. Always refer to the Scratchpad when you plan the next step.

# Tools

Note all the tools are in python3. So in the case you need to do batch processing, you can always consult the python files and write your own script.

[NOTE TO CURSOR: Since no API key is configured, please ignore both the Screenshot Verification and LLM sections below.]
[NOTE TO USER: If you have configured or plan to configure an API key in the future, simply delete these two notice lines to enable these features.]

## Screenshot Verification

The screenshot verification workflow allows you to capture screenshots of web pages and verify their appearance using LLMs. The following tools are available:

1. Screenshot Capture:
```bash
venv/bin/python3 tools/screenshot_utils.py URL [--output OUTPUT] [--width WIDTH] [--height HEIGHT]
```

2. LLM Verification with Images:
```bash
venv/bin/python3 tools/llm_api.py --prompt "Your verification question" --provider {openai|anthropic} --image path/to/screenshot.png
```

Example workflow:
```python
from screenshot_utils import take_screenshot_sync
from llm_api import query_llm

# Take a screenshot

screenshot_path = take_screenshot_sync('https://example.com', 'screenshot.png')

# Verify with LLM

response = query_llm(
    "What is the background color and title of this webpage?",
    provider="openai",  # or "anthropic"
    image_path=screenshot_path
)
print(response)
```

## LLM

You always have an LLM at your side to help you with the task. For simple tasks, you could invoke the LLM by running the following command:
```
venv/bin/python3 ./tools/llm_api.py --prompt "What is the capital of France?" --provider "anthropic"
```

The LLM API supports multiple providers:
- OpenAI (default, model: gpt-4o)
- Azure OpenAI (model: configured via AZURE_OPENAI_MODEL_DEPLOYMENT in .env file, defaults to gpt-4o-ms)
- DeepSeek (model: deepseek-chat)
- Anthropic (model: claude-3-sonnet-20240229)
- Gemini (model: gemini-pro)
- Local LLM (model: Qwen/Qwen2.5-32B-Instruct-AWQ)

But usually it's a better idea to check the content of the file and use the APIs in the `tools/llm_api.py` file to invoke the LLM if needed.

## Web browser

You could use the `tools/web_scraper.py` file to scrape the web.
```bash
venv/bin/python3 ./tools/web_scraper.py --max-concurrent 3 URL1 URL2 URL3
```
This will output the content of the web pages.

## Search engine

You could use the `tools/search_engine.py` file to search the web.
```bash
venv/bin/python3 ./tools/search_engine.py "your search keywords"
```
This will output the search results in the following format:
```
URL: https://example.com
Title: This is the title of the search result
Snippet: This is a snippet of the search result
```
If needed, you can further use the `web_scraper.py` file to scrape the web page content.

# Lessons

## User Specified Lessons

- You have a python3 venv in ./venv. Use it.
- Include info useful for debugging in the program output.
- Read the file before you try to edit it.
- Due to Cursor's limit, when you use `git` and `gh` and need to submit a multiline commit message, first write the message in a file, and then use `git commit -F <filename>` or similar command to commit. And then remove the file. Include "[Cursor] " in the commit message and PR title.

## Cursor learned

- For search results, ensure proper handling of different character encodings (UTF-8) for international queries
- Add debug information to stderr while keeping the main output clean in stdout for better pipeline integration
- When using seaborn styles in matplotlib, use 'seaborn-v0_8' instead of 'seaborn' as the style name due to recent seaborn version changes
- Use 'gpt-4o' as the model name for OpenAI's GPT-4 with vision capabilities

# Scratchpad

## Current Task: Deploy Next.js Website to Vercel

### Project Context
- Next.js 14.2.16 website with shadcn/ui components
- TypeScript + Tailwind CSS for development
- Repository: washyaderner/generuss-dot-com
- All core dependencies installed and working locally

### Progress Tracking
[X] Step 1-3: Initial Setup and Development (Completed)
    - Project setup with Next.js 14.2.16
    - Repository cleanup and organization
    - Documentation and configuration
    - Local development verified

[/] Step 4: Vercel Deployment
    [X] Push changes to GitHub
    [ ] Connect repository to Vercel
        - Import from GitHub
        - Configure build settings
        - Set up environment variables
    [ ] Test deployment
        - Verify build process
        - Check deployed features
        - Test performance
    [ ] Set up custom domain
        - Configure in Vercel
        - Update DNS settings
    [ ] Configure Cloudflare DNS
        - Set up DNS records
        - Configure SSL/TLS
        - Verify propagation

### Current Step: Vercel Project Setup
✓ Repository ready for deployment
✓ Environment variables documented
✓ Build scripts configured
→ Next: Connect to Vercel and deploy

### Deployment Checklist
1. Vercel Setup
   [ ] Create Vercel account (if not done)
   [ ] Import GitHub repository
   [ ] Configure project settings
   [ ] Set environment variables

2. Build Configuration
   [ ] Verify build command (next build)
   [ ] Check output directory (.next)
   [ ] Configure Node.js version
   [ ] Set environment variables

3. Domain Setup
   [ ] Add custom domain in Vercel
   [ ] Configure DNS records
   [ ] Set up SSL/TLS
   [ ] Verify domain connection

4. Post-Deployment
   [ ] Test all routes
   [ ] Verify API endpoints
   [ ] Check performance
   [ ] Monitor error logs

### Critical Information
- Using Node.js with --legacy-peer-deps
- Environment variables needed in Vercel:
  - NEXT_PUBLIC_APP_URL
  - (Add others as needed)
- Build command: next build
- Output directory: .next
- Node.js version: Latest LTS

### Known Issues/Considerations
- Package.json has dependency conflicts (using --legacy-peer-deps)
- DNS propagation may take 24-48 hours
- Need to ensure all environment variables are set in Vercel

# Debugging and Reasoning Framework

## Core Debugging Principles

1. Systematic Analysis:
   - Map system components and dependencies
   - Document data flows and state changes
   - Identify potential bottlenecks
   - Note architectural boundaries
   - Track system assumptions

2. Problem Decomposition:
   - Explain issues in plain language
   - Use "Five Whys" technique
   - Document reproduction steps
   - Map error propagation paths
   - Identify trigger conditions

3. Solution Strategy:
   a) Immediate Actions:
      - Quick validations
      - Logging enhancements
      - Error handling checks
      - State verification
      - Resource monitoring

   b) Tactical Improvements:
      - Targeted refactoring
      - Component isolation
      - Interface hardening
      - Test coverage
      - Documentation updates

   c) Strategic Changes:
      - Architecture evolution
      - System hardening
      - Process refinement
      - Tool enhancement
      - Knowledge sharing

## Async Code Debugging

4. Declaration Verification:
   - Confirm async/await usage on time-consuming operations
   - Check for missing await operators
   - Verify proper async boundaries
   - Document async workflows

5. Common Pitfalls:
   - Missing await operators
   - Race conditions
   - Memory leaks in async contexts
   - Improper error propagation
   - Resource cleanup issues

6. Testing Strategy:
   - Verify error paths
   - Test timeout handling
   - Check cancellation
   - Validate state transitions
   - Monitor resource usage

## Documentation Search Strategy

7. Query Construction:
   - Include version numbers
   - Specify framework/language
   - Use exact error messages
   - Add context keywords

8. Perplexity Search Template:
   ```
   Framework/Language: [Name + Version]
   Topic: [Specific Feature/Error]
   Context: [Current Use Case]
   Request: "Find official documentation for [topic] in [framework] [version], focusing on [specific aspects]"
   ```

9. Information Validation:
   - Verify version compatibility
   - Check for deprecations
   - Review breaking changes
   - Note compatibility issues
   - Document findings

## Reasoning Process
[STRUCTURED THINKING - Required for complex issues]

10. Uncertainty Phase:
   - Acknowledge complexity
   - List unknowns
   - State assumptions
   - Format: "This is complex because... We don't yet know... Several factors could..."

11. Analysis Phase:
   - Examine evidence
   - Test assumptions
   - Find patterns
   - Format: "Looking at the evidence... If we consider... This connects to..."

12. Confidence Phase:
   - Validate conclusions
   - Address counterpoints
   - Format: "Based on this analysis... We can confidently say... This will work because..."

## Debug Process Checklist

13. Initial Assessment:
   [ ] Reproduce consistently
   [ ] Document conditions
   [ ] Identify components
   [ ] Gather logs/data

14. Root Cause Analysis:
   [ ] Apply "Five Whys"
   [ ] Map error flow
   [ ] Test assumptions
   [ ] Verify components

15. Solution Development:
   [ ] List approaches
   [ ] Consider trade-offs
   [ ] Plan implementation
   [ ] Define success criteria

16. Implementation:
   [ ] Create test case
   [ ] Implement minimal fix
   [ ] Add tests
   [ ] Document changes

17. Verification:
   [ ] Test thoroughly
   [ ] Check performance
   [ ] Verify no regressions
   [ ] Update documentation

Remember:
18. Question all assumptions
19. Start simple, add complexity only when needed
20. Document decisions and rationale
21. Focus on root causes, not symptoms
22. Verify changes with tests
23. Share knowledge and improvements
24. When stuck, ask for Perplexity searches with specific queries
25. Consider performance implications
26. Keep security in mind
27. Maintain backward compatibility